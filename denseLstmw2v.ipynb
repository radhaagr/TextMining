{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import  os\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import cgi\n",
    "from datacleaner import *\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = 1\n",
    "os.listdir(os.getcwd())\n",
    "\n",
    "EMBEDDING_DIM=200\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'tweet_remove_blank.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joy\\n': 25278, 'disgust\\n': 23739, 'anger\\n': 23569, 'surprise\\n': 24061, 'fear\\n': 23450, 'sad\\n': 20534}\n",
      "Max vocab length :  29\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "listTokens = []\n",
    "maxvocab = 0\n",
    "diclabel = {}\n",
    "listlabels = []\n",
    "with open(input_file,  encoding='utf-8') as f:\n",
    "    for i,line in enumerate (f):\n",
    "\n",
    "        count = count + 1\n",
    "        \n",
    "        if (count == 1):\n",
    "            continue\n",
    "\n",
    "        \n",
    "        id, text, label  = tweettoidtextlabel(line)\n",
    "        text = text.strip()\n",
    "        if label in diclabel:\n",
    "            val = diclabel[label]\n",
    "            val = val +1 \n",
    "            diclabel[label] =  val\n",
    "        else:\n",
    "            diclabel[label] = 0\n",
    "        \n",
    "        #print( \"original tweet : [ \" , count ,\" ] \" ,  text )\n",
    "        htmltext = removehtmlentities(text)\n",
    "        urltext  = removeurl(htmltext)\n",
    "        attext = removeat(urltext)\n",
    "        triggertext =  removetriggerword(attext)\n",
    "        expandabbrevation(triggertext)\n",
    "        finaltoken = removestopword(tokenizeString(triggertext))\n",
    "        curtotal = len(finaltoken)\n",
    "        if ( curtotal > maxvocab):\n",
    "            maxvocab = curtotal\n",
    "        finaltoken = \" \".join(finaltoken)\n",
    "\n",
    "        listTokens.append(finaltoken )\n",
    "        listlabels.append(label)\n",
    "        #print( \"tweet set : [ \" , count ,\" ] \" ,  finaltoken )\n",
    "        #print('')\n",
    "        #if ( count == 100):\n",
    "        #    break\n",
    "\n",
    "print (diclabel)\n",
    "print (\"Max vocab length : \" ,maxvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 3 ... 4 2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "labelen = preprocessing.LabelEncoder()\n",
    "le = labelen.fit(listlabels)\n",
    "translabel = le.transform(listlabels)\n",
    "print (translabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "tmp_file   = get_tmpfile(\"test_word2vec.txt\")\n",
    "glove2word2vec(\"glove.twitter.27B.200d.txt\", tmp_file)\n",
    "word2vec = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81059 unique tokens\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(listTokens)\n",
    "sequences_1 = tokenizer.texts_to_sequences(listTokens)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "data_1 = pad_sequences(sequences_1, maxlen=maxvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "labels = np.array(translabel)\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "\n",
    "#print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack(data_1[idx_train])\n",
    "labels_train = labels[idx_train]\n",
    "\n",
    "data_1_val = np.vstack(data_1[idx_val])\n",
    "labels_val = labels[idx_val]\n",
    "re_weight = True\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 29)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 29, 200)           16212000  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 196)               311248    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 196)               784       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 105)               20685     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 105)               420       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 636       \n",
      "=================================================================\n",
      "Total params: 16,545,773\n",
      "Trainable params: 16,545,171\n",
      "Non-trainable params: 602\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "lstm_out = 196\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "embedding_layer =  Embedding(nb_words, EMBEDDING_DIM,weights=[embedding_matrix],  input_length=maxvocab)\n",
    "lstm_layer = LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)\n",
    "sequence_1_input = Input(shape=(maxvocab,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "merged  = Dropout(rate_drop_dense)(x1)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation='relu')(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(6, activation='softmax')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input],  outputs=preds)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics =['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140637,)\n",
      "(140637,)\n",
      "(140637, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#labels  = np_utils.to_categorical(labels, 6)\n",
    "print (translabel.shape)\n",
    "getar = np.array(translabel)\n",
    "print (getar.shape)\n",
    "newl = np_utils.to_categorical(getar, 6)\n",
    "print (newl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94226, 29) (94226, 6)\n",
      "(46411, 29) (46411, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split( data_1 ,newl, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 32\n",
    "\n",
    "#model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94226 samples, validate on 46411 samples\n",
      "Epoch 1/5\n",
      "94226/94226 [==============================] - 406s 4ms/step - loss: 1.5165 - categorical_accuracy: 0.4084 - val_loss: 1.3083 - val_categorical_accuracy: 0.4988\n",
      "Epoch 2/5\n",
      "94226/94226 [==============================] - 413s 4ms/step - loss: 1.2374 - categorical_accuracy: 0.5368 - val_loss: 1.2833 - val_categorical_accuracy: 0.5118\n",
      "Epoch 3/5\n",
      "94226/94226 [==============================] - 405s 4ms/step - loss: 1.0259 - categorical_accuracy: 0.6260 - val_loss: 1.3493 - val_categorical_accuracy: 0.5080\n",
      "Epoch 4/5\n",
      "94226/94226 [==============================] - 405s 4ms/step - loss: 0.8143 - categorical_accuracy: 0.7082 - val_loss: 1.4840 - val_categorical_accuracy: 0.4987\n",
      "Epoch 5/5\n",
      "94226/94226 [==============================] - 405s 4ms/step - loss: 0.6683 - categorical_accuracy: 0.7621 - val_loss: 1.6522 - val_categorical_accuracy: 0.4878\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, \\\n",
    "        validation_data=(X_test, Y_test ), \\\n",
    "        epochs=5, batch_size=50, shuffle=True )\n",
    "\n",
    "\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels 28757\n"
     ]
    }
   ],
   "source": [
    "test_labels = []\n",
    "label_file = 'test.labels'\n",
    "with open(label_file,  encoding='utf-8') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        test_labels.append(line.strip())\n",
    "          \n",
    "print (\"labels\" ,len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsequence = []\n",
    "test_file = \"test.csv\"\n",
    "count = 0\n",
    "with open(test_file,  encoding='utf-8') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        text = line.strip()\n",
    "        htmltext = removehtmlentities(text)\n",
    "        urltext  = removeurl(htmltext)\n",
    "        attext = removeat(urltext)\n",
    "        triggertext =  removetriggerword(attext)\n",
    "        expandabbrevation(triggertext)\n",
    "        finaltoken = removestopword(tokenizeString(triggertext))\n",
    "        curtotal = len(finaltoken)\n",
    "        testsequence.append(finaltoken )\n",
    "        #if( count == 100 ):\n",
    "        #    break\n",
    "        #count = count + 1\n",
    "#print (\"Testdata\" , testsequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(testsequence)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(testsequence)\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=maxvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28757/28757 [==============================] - 7s 237us/step\n",
      "[1 2 4 ... 4 5 2]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_data_1, batch_size=8192, verbose=1)\n",
    "print ( preds.argmax(axis=1))\n",
    "result = preds.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1116  793  829  872  427  757]\n",
      " [1085  758 1011  604  498  838]\n",
      " [1499  701  924  536  466  665]\n",
      " [1511  763  852  846  469  805]\n",
      " [1050  665  720  754  373  778]\n",
      " [1356  729  951  557  502  697]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "labelen = preprocessing.LabelEncoder()\n",
    "le = labelen.fit(test_labels)\n",
    "#print (list(le.classes_))\n",
    "trans_testlabel = le.transform(test_labels)\n",
    "\n",
    "x = confusion_matrix(trans_testlabel, result)\n",
    "print (x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.15      0.23      0.18      4794\n",
      "     disgust       0.17      0.16      0.16      4794\n",
      "        fear       0.17      0.19      0.18      4791\n",
      "         joy       0.20      0.16      0.18      5246\n",
      "         sad       0.14      0.09      0.11      4340\n",
      "    surprise       0.15      0.15      0.15      4792\n",
      "\n",
      "   micro avg       0.16      0.16      0.16     28757\n",
      "   macro avg       0.16      0.16      0.16     28757\n",
      "weighted avg       0.17      0.16      0.16     28757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(trans_testlabel, result, target_names=list(le.classes_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'sad', 'surprise']\n",
      "[4 2 5 ... 5 3 3]\n",
      "(28757,)\n",
      "(28757, 6)\n",
      "['disgust', 'anger', 'fear', 'joy', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "labelen = preprocessing.LabelEncoder()\n",
    "le = labelen.fit(test_labels)\n",
    "#print (list(le.classes_))\n",
    "trans_testlabel = le.transform(test_labels)\n",
    "#print (trans_testlabel)\n",
    "#print (trans_testlabel.shape)\n",
    "category_test_label = np_utils.to_categorical(trans_testlabel, 6)\n",
    "\n",
    "#print (category_test_label.shape)\n",
    "#print (list(le.inverse_transform([1,0, 2, 3,4,5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad', 'fear', 'surprise', 'fear', 'fear', 'fear', 'disgust', 'fear', 'fear', 'anger']\n",
      "[4 2 5 2 2 2 1 2 2 0]\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#print  (test_labels[0:10])\n",
    "#print (trans_testlabel[0:10] )\n",
    "#print ( category_test_label[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28757/28757 [==============================] - 6s 214us/step\n",
      "Accuracy: 17.14%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_data_1, category_test_label, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "x = confusion_matrix(y_true, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
